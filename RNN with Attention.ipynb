{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN for Machine Translation\n",
    "## Initial Data Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we will read in the data for the Vietnamese and Chinese to Engish corpuses, build a token2id and char2id mapping, vocabularies and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN for Machine Translation\n",
    "Initial Data Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import pickle as pkl\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import io\n",
    "from collections import Counter\n",
    "import sacrebleu\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Global Variables\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in pre-trained fasttext embeddings for the three languages\n",
    "### Building loaded embeddings, token2id, id2token and ordered words for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words embedded is 400,000\n",
      "Total number of words embedded is 292,168\n",
      "Total number of words embedded is 332,647\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-trained Word Vectors\n",
    "def load_embeddings(word2vec, word2id, embedding_dim):\n",
    "    embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embeddings[index] = word2vec[word]\n",
    "\n",
    "        except KeyError:\n",
    "            embeddings[index] = np.random.normal(scale=0.6, size=(300,))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_vectors(fname, num_vecs=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "        if num_vecs is None:\n",
    "            pass\n",
    "        else:\n",
    "            if len(data) + 1 > num_vecs:\n",
    "                break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_dictionary(tokens, vocab_size_limit):\n",
    "    token_counter = Counter()\n",
    "    for token in tokens:\n",
    "        token_counter[token] += 1\n",
    "\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size_limit))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(4, 4 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>','<sos>','<eos>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    token2id['<sos>'] = SOS_IDX\n",
    "    token2id['<eos>'] = EOS_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#Load Word Embeddings\n",
    "\n",
    "path= '../pretrained_embeddings/'\n",
    "\n",
    "en_loaded_embeddings = load_vectors(path +'wiki.en.vec',400000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(en_loaded_embeddings)))\n",
    "\n",
    "vi_loaded_embeddings = load_vectors(path+'wiki.vi.vec',400000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(vi_loaded_embeddings)))\n",
    "\n",
    "zh_loaded_embeddings = load_vectors(path+'wiki.zh.vec',400000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(zh_loaded_embeddings)))\n",
    "\n",
    "\n",
    "#Create token2Id, token2Id\n",
    "en_token2id, en_id2token = data_dictionary(list(en_loaded_embeddings.keys()), 400000)\n",
    "vi_token2id, vi_id2token = data_dictionary(list(vi_loaded_embeddings.keys()), 400000)\n",
    "zh_token2id, zh_id2token = data_dictionary(list(zh_loaded_embeddings.keys()), 400000)\n",
    "\n",
    "\n",
    "#Create Emedding Matrix\n",
    "embedding_size = 300\n",
    "en_loaded_embeddings=load_embeddings(en_loaded_embeddings,en_token2id,embedding_size)\n",
    "vi_loaded_embeddings=load_embeddings(vi_loaded_embeddings,vi_token2id,embedding_size)\n",
    "zh_loaded_embeddings=load_embeddings(zh_loaded_embeddings,zh_token2id,embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vi -> En | Training Examples: 133317\n",
      "Vi -> En | Training Examples: 133317 \n",
      "\n",
      "Vi -> En | Validation Examples: 1268\n",
      "Vi -> En | Validation Examples: 1268 \n",
      "\n",
      "Vi -> En | Testing Examples: 1553\n",
      "Vi -> En | Testing Examples: 1553 \n",
      "\n",
      "Zh -> En | Training Examples: 213377\n",
      "Zh -> En | Training Examples: 213377 \n",
      "\n",
      "Zh -> En | Validation Examples: 1261\n",
      "Zh -> En | Validation Examples: 1261 \n",
      "\n",
      "Zh -> En | Testing Examples: 1397\n",
      "Zh -> En | Testing Examples: 1397 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading in the Vietnamese -> En datasets\n",
    "path1= '../project_data/en-vi/'\n",
    "\n",
    "train_vi_en = []\n",
    "with open(path1 +'train.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "train_vi_vi = []\n",
    "with open(path1 + 'train.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_vi_vi.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_vi_en = []\n",
    "with open(path1 + 'dev.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_vi_vi = []\n",
    "with open(path1 +'dev.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_vi_vi.append(line.strip().lower().split(' '))\n",
    "        \n",
    "test_vi_en = []\n",
    "with open(path1 +'test.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "test_vi_vi = []\n",
    "with open(path1 + 'test.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_vi_vi.append(line.strip().lower().split(' '))\n",
    "        \n",
    "        \n",
    "        \n",
    "#Loading in the Chinese -> En datasets\n",
    "path1= '../project_data/en-zh/'\n",
    "\n",
    "train_zh_en = []\n",
    "with open(path1 +'train.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "train_zh_zh = []\n",
    "with open(path1 + 'train.tok.zh') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_zh_zh.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_zh_en = []\n",
    "with open(path1 + 'dev.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_zh_zh = []\n",
    "with open(path1 +'dev.tok.zh') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_zh_zh.append(line.strip().lower().split(' '))\n",
    "        \n",
    "test_zh_en = []\n",
    "with open(path1 +'test.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "test_zh_zh = []\n",
    "with open(path1 + 'test.tok.zh') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_zh_zh.append(line.strip().lower().split(' '))\n",
    "    \n",
    "    \n",
    "#Sanity Checking\n",
    "print(\"Vi -> En | Training Examples: \"+str(len(train_vi_en)))\n",
    "print(\"Vi -> En | Training Examples: \"+str(len(train_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Vi -> En | Validation Examples: \"+str(len(val_vi_en)))\n",
    "print(\"Vi -> En | Validation Examples: \"+str(len(val_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Vi -> En | Testing Examples: \"+str(len(test_vi_en)))\n",
    "print(\"Vi -> En | Testing Examples: \"+str(len(test_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Training Examples: \"+str(len(train_zh_en)))\n",
    "print(\"Zh -> En | Training Examples: \"+str(len(train_zh_zh)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Validation Examples: \"+str(len(val_zh_en)))\n",
    "print(\"Zh -> En | Validation Examples: \"+str(len(val_zh_zh)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Testing Examples: \"+str(len(test_zh_en)))\n",
    "print(\"Zh -> En | Testing Examples: \"+str(len(test_zh_zh)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preserve original data for evaluation\n",
    "train_vi_en_orig = train_vi_en\n",
    "train_vi_vi_orig = train_vi_vi\n",
    "val_vi_en_orig = val_vi_en\n",
    "val_vi_vi_orig = val_vi_vi\n",
    "test_vi_en_orig = test_vi_en\n",
    "test_vi_vi_orig = test_vi_vi\n",
    "\n",
    "train_zh_en_orig = train_zh_en\n",
    "train_zh_zh_orig = train_zh_zh\n",
    "val_zh_en_orig = val_zh_en\n",
    "val_zh_zh_orig = val_zh_zh\n",
    "test_zh_en_orig = test_zh_en\n",
    "test_zh_zh_orig = test_zh_zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VI_EN_MAX_LENGTH = int(np.percentile([len(sentence) for sentence in train_vi_en+train_vi_vi], 90))+1\n",
    "ZH_EN_MAX_LENGTH = int(np.percentile([len(sentence) for sentence in train_zh_en+train_zh_zh], 90))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_tokens(sentence, language, translator):\n",
    "    if language== 'English':\n",
    "        token2id = en_token2id\n",
    "    elif language== 'Vietnamese':\n",
    "        token2id = vi_token2id\n",
    "    elif language== 'Chinese':\n",
    "        token2id = zh_token2id\n",
    "    tokens = [token2id[token] if token in token2id else UNK_IDX for token in sentence]\n",
    "    if translator == 'vi':\n",
    "        max_len = VI_EN_MAX_LENGTH-1\n",
    "    elif translator == 'zh':\n",
    "        max_len = ZH_EN_MAX_LENGTH-1\n",
    "    tokens=tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "def encoding_dataset(dataset, language, translator):\n",
    "    data = [encoding_tokens(tokens, language, translator) for tokens in dataset] \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vi_en = encoding_dataset(train_vi_en, 'English', 'vi')\n",
    "train_vi_vi = encoding_dataset(train_vi_vi, 'Vietnamese', 'vi')\n",
    "test_vi_en = encoding_dataset(test_vi_en, 'English', 'vi')\n",
    "test_vi_vi = encoding_dataset(test_vi_vi, 'Vietnamese', 'vi')\n",
    "val_vi_en = encoding_dataset(val_vi_en, 'English', 'vi')\n",
    "val_vi_vi = encoding_dataset(val_vi_vi, 'Vietnamese', 'vi')\n",
    "\n",
    "train_zh_en = encoding_dataset(train_zh_en, 'English', 'zh')\n",
    "train_zh_zh = encoding_dataset(train_zh_zh, 'Chinese', 'zh')\n",
    "test_zh_en = encoding_dataset(test_zh_en, 'English', 'zh')\n",
    "test_zh_zh = encoding_dataset(test_zh_zh, 'Chinese', 'zh')\n",
    "val_zh_en = encoding_dataset(val_zh_en, 'English', 'zh')\n",
    "val_zh_zh = encoding_dataset(val_zh_zh, 'Chinese', 'zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class translationDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        self.data_list=data_list\n",
    "        self.target_list=target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        data = self.data_list[key][:MAX_SAMPLE_LENGTH]\n",
    "        label = self.target_list[key][:MAX_SAMPLE_LENGTH]\n",
    "        return [data, len(data), label, len(label)]\n",
    "\n",
    "def translation_collate_func(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for datum in batch:\n",
    "        padded_data = np.pad(np.array(datum[0]+[EOS_IDX]), \n",
    "                                pad_width=((0,MAX_SAMPLE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_data)\n",
    "        padded_label = np.pad(np.array(datum[2]+[EOS_IDX]), \n",
    "                                pad_width=((0,MAX_SAMPLE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        label_list.append(padded_label)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VI -> EN | dataloaders\n",
    "MAX_SAMPLE_LENGTH = VI_EN_MAX_LENGTH\n",
    "\n",
    "vi_en_train_dataset = translationDataset(train_vi_vi, train_vi_en)\n",
    "vi_en_train_loader = torch.utils.data.DataLoader(dataset=vi_en_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "vi_en_val_dataset = translationDataset(val_vi_vi, val_vi_en)\n",
    "vi_en_val_loader = torch.utils.data.DataLoader(dataset=vi_en_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "vi_en_test_dataset = translationDataset(test_vi_vi, test_vi_en)\n",
    "vi_en_test_loader = torch.utils.data.DataLoader(dataset=vi_en_test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZH -> EN | dataloaders\n",
    "MAX_SAMPLE_LENGTH = ZH_EN_MAX_LENGTH\n",
    "\n",
    "zh_en_train_dataset = translationDataset(train_zh_zh, train_zh_en)\n",
    "zh_en_train_loader = torch.utils.data.DataLoader(dataset=zh_en_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "zh_en_val_dataset = translationDataset(val_zh_zh, val_zh_en)\n",
    "zh_en_val_loader = torch.utils.data.DataLoader(dataset=zh_en_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "zh_en_test_dataset = translationDataset(test_zh_zh, test_zh_en)\n",
    "zh_en_test_loader = torch.utils.data.DataLoader(dataset=zh_en_test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points, string):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(points)\n",
    "    plt.title(string)\n",
    "    plt.savefig((string+'.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, language, drop_rate=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.language = language\n",
    "        if language == 'Vietnamese':\n",
    "             self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vi_loaded_embeddings), freeze=True)\n",
    "        elif language == 'Chinese':\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vi_loaded_embeddings), freeze=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = self.dropout(embedded)\n",
    "        output, hidden = self.gru(hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(en_loaded_embeddings), freeze=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        #self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) \n",
    "        self.gru = nn.GRU(self.hidden_size + self.embedding_size, self.hidden_size, dropout=dropout_p)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Get the embedding of the current input\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # S=1 x B x N\n",
    "        embedded = self.dropout(embedded)\n",
    "        print(\"embedded size\", embedded.size())\n",
    "        print(\"hidden size\", hidden.size())\n",
    "        print(\"encoder outputs\", encoder_outputs.size())\n",
    "        # Get weights from dot product\n",
    "        attn_weights = torch.bmm(hidden, encoder_outputs.view(1, -1, self.hidden_size).transpose(1,2))\n",
    "\n",
    "        # Softmax to normalize\n",
    "        attn_weights = F.softmax(attn_weights)\n",
    "        print(\"attn weights\", attn_weights.size())\n",
    "        \n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = attn_weights.bmm(encoder_outputs.view(1, -1, self.hidden_size)) \n",
    "        print(\"context\", context.size())\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, context), 2)\n",
    "        print(\"rnn input\", rnn_input.size())\n",
    "        print(\"hidden size\", hidden.size())\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        print(\"output\", output.size())\n",
    "        #output = output.squeeze(0) \n",
    "        #output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    " \n",
    "        print(\"output\", output.size())\n",
    "        print(\"hidden\", hidden.size())\n",
    "        print(\"attn\", attn_weights.size())\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAttn(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_tensor = input_variable#.transpose(0,1)\n",
    "    target_tensor = target_variable#.transpose(0,1)\n",
    "    \n",
    "    max_length = input_tensor.size(0)\n",
    "    #batch_size = input_tensor.size(1)\n",
    "    #vocab_size = len(en_token2id)\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "   # encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "   # encoder_outputs = encoder_output[0,0]\n",
    "\n",
    "    decoder_input = torch.tensor(SOS_IDX, device=device)\n",
    "    #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    # use_teacher_forcing = True\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            print(\"target size\",target_tensor[di].size())\n",
    "            decoder_output, decoder_hidden, hidden_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            print(target_tensor[di].size())\n",
    "            decoder_output, decoder_hidden, hidden_weights = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = torch.cat(topi) \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / batch_size\n",
    "\n",
    "def trainAttnIters(loader, encoder, decoder, n_iters, max_length, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            input_tensor = data\n",
    "            target_tensor = labels\n",
    "\n",
    "            loss = trainAttn(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "    showPlot(plot_losses, 'Vietnamese') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target size torch.Size([45])\n",
      "embedded size torch.Size([1, 1, 300])\n",
      "hidden size torch.Size([1, 1, 256])\n",
      "encoder outputs torch.Size([64, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-99de1936e45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Vietnamese'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_token2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVI_EN_MAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainAttnIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi_en_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVI_EN_MAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-e9f21a04bc2d>\u001b[0m in \u001b[0;36mtrainAttnIters\u001b[0;34m(loader, encoder, decoder, n_iters, max_length, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             loss = trainAttn(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 83\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-e9f21a04bc2d>\u001b[0m in \u001b[0;36mtrainAttn\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target size\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             decoder_output, decoder_hidden, hidden_weights = decoder(\n\u001b[0;32m---> 41\u001b[0;31m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-6c5dd039ac59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Get weights from dot product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m        \u001b[0;31m# attn_weights = torch.bmm(hidden, encoder_outputs.view(1, -1, self.hidden_size).transpose(1,2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Training Model\n",
    "teacher_forcing_ratio = 1\n",
    "hidden_size = 256\n",
    "embedding_size = 300\n",
    "encoder1 = EncoderRNN(embedding_size, hidden_size, 'Vietnamese').to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(embedding_size, hidden_size, len(en_token2id), VI_EN_MAX_LENGTH, dropout_p=0.1)\n",
    "trainAttnIters(vi_en_val_loader, encoder1, attn_decoder1, n_iters=10, max_length=VI_EN_MAX_LENGTH, print_every=1,plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    if lang == 'Vietnamese':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in vi_token2id:\n",
    "                out.append(vi_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    elif lang == 'English':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in en_token2id:\n",
    "                out.append(en_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    elif lang == 'Chinese':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in zh_token2id:\n",
    "                out.append(zh_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    return out\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_IDX)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang):\n",
    "    with torch.no_grad():   \n",
    "        \n",
    "        #for i, (data, labels) in enumerate(vi_en_train_loader):\n",
    "        #    input_tensor = data\n",
    "        \n",
    "       \n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        max_length = input_tensor.size(0)\n",
    "        input_tensor = input_tensor.transpose(0,1)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        #print(input_length)\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        #encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "        #encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
    "\n",
    "        #for ei in range(input_length):\n",
    "        #    encoder_output, encoder_hidden = encoder(\n",
    "        #        input_tensor)\n",
    "        #    encoder_outputs[ei] = encoder_output[0,0]\n",
    "            \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor)\n",
    "        encoder_outputs = encoder_output[0,0]\n",
    "\n",
    "        #decoder_input = torch.tensor([[SOS_IDX]] * batch_size)\n",
    "        decoder_input = input_tensor[0,:]\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        #decoder_attentions = torch.zeros(max_length, max_length)\n",
    "                \n",
    " #       for di in range(max_length):\n",
    " #           decoder_output, decoder_hidden = decoder(\n",
    "  #             decoder_input, decoder_hidden, encoder_hidden)\n",
    "          #  print(decoder_output)\n",
    "          #  print(decoder_hidden)\n",
    "  #          topv, topi = decoder_output.topk(1, di)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "  #          decoder_input = topi.squeeze().detach()\n",
    "  #          print(decoder_input[di].item())\n",
    "  #          if decoder_input[di].item()== EOS_IDX:\n",
    "  #              decoded_words.append('<EOS>')\n",
    "  #              break\n",
    "  #          else:\n",
    "  #              decoded_words.append(en_id2token[decoder_input[di].item()])\n",
    "  #          print(decoded_words)\n",
    "  #          decoder_input = decoder_input[di]\n",
    "\n",
    "\n",
    "#add beam search, using for loops, validation or test time\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "               decoder_input, decoder_hidden, encoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            if decoder_input[di].item() == EOS_IDX:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(en_id2token[decoder_input[di].item()])\n",
    "                \n",
    "        return decoded_words#, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def evaluateRandomly(encoder, decoder, language, n=10):\n",
    "    for i in range(n):\n",
    "        if language == 'Vietnamese':\n",
    "            index = randint(0, len(train_vi_vi_orig))\n",
    "            sentence1 = train_vi_vi_orig[index] \n",
    "            sentence2 = train_vi_en_orig[index]\n",
    "        elif language == 'Chinese':\n",
    "            index = randint(0, len(train_zh_zh_orig))\n",
    "            sentence1 = train_zh_zh_orig[index]\n",
    "            sentence2 = train_zh_en_orig[index]\n",
    "        \n",
    "        print('>', sentence1)\n",
    "        print('=', sentence2)\n",
    "        output_words = evaluate(encoder, decoder, sentence1, language)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['\"', 'con_người_ta', 'là', 'một', 'công_việc', 'còn', 'đang', 'dang_dở', 'lại', 'lầm', 'tưởng', 'rằng', 'mình', 'đã', 'xong_xuôi', '.', '\"', 'dan', 'gilbert', 'chia_sẻ', 'công_trình', 'nghiên_cứu', 'gần_đây', 'của_ông', 'về', 'một', 'hiện_tượng', 'được', 'gọi', 'là', '\"', 'đoạn', 'kết', 'của', 'lịch_sử', 'ảo_tưởng', ',', '\"', 'ở', 'đó', 'chúng_ta', 'tưởng', 'rằng', 'con_người', 'hiện_tại', 'của', 'chúng_ta', 'sẽ', 'cứ', 'thế', 'này', 'mãi_mãi', '.', 'kết', 'qủa', 'nghiên_cứu', 'chỉ', 'ra', 'rằng', 'chẳng', 'phải', 'thế', 'đâu', '.']\n",
      "= ['&quot;', 'human', 'beings', 'are', 'works', 'in', 'progress', 'that', 'mistakenly', 'think', 'they', '&apos;re', 'finished', '.', '&quot;', 'dan', 'gilbert', 'shares', 'recent', 'research', 'on', 'a', 'phenomenon', 'he', 'calls', 'the', '&quot;', 'end', 'of', 'history', 'illusion', ',', '&quot;', 'where', 'we', 'somehow', 'imagine', 'that', 'the', 'person', 'we', 'are', 'right', 'now', 'is', 'the', 'person', 'we', '&apos;ll', 'be', 'for', 'the', 'rest', 'of', 'time', '.', 'hint', ':', 'that', '&apos;s', 'not', 'the', 'case', '.']\n",
      "< <pad> <pad>\n",
      "\n",
      "> ['nhưng', 'anh', 'ta', 'có_thể', 'nhớ', 'tất_cả', 'những', 'nhu_cầu', 'của', 'bản_thân', ',', 'sẽ', 'như_vậy', ',', 'đã', 'như_vậy', '.']\n",
      "= ['but', 'he', 'could', 'memorize', 'all', 'the', 'needs', 'of', 'all', 'the', 'beings', 'that', 'he', 'is', ',', 'he', 'will', ',', 'he', 'did', '.']\n",
      "< <pad> <pad>\n",
      "\n",
      "> ['nhưng', 'đó', 'không', 'hẳn_là', 'điện', 'hợp', 'hạch', '.', 'đó', 'chỉ', 'là', 'khiến', 'phản_ứng', 'hợp', 'hạch', 'diễn', 'ra', '.']\n",
      "= ['but', 'that', '&apos;s', 'not', 'really', 'fusion', 'power', '.', 'that', '&apos;s', 'just', 'making', 'some', 'fusion', 'happen', '.']\n",
      "< <pad> <pad>\n",
      "\n",
      "> ['vì_vậy', ',', 'chúng_tôi', 'đã', 'đưa', 'bà', 'ấy', 'trở_về', 'nhà', ',', 'và', 'tôi', 'đã', 'cầu', 'nguyền', 'rằng', 'dầu', 'đừng_có', 'tràn', 'vào', 'bờ_biển', 'của', 'bà', 'ấy', 'trước_khi', 'bà', 'ấy', 'qua_đời', '.']\n",
      "= ['so', 'we', 'brought', 'her', 'home', ',', 'and', 'i', 'prayed', 'that', 'the', 'oil', 'wouldn', '&apos;t', 'wash', 'up', 'on', 'her', 'beach', 'before', 'she', 'died', '.']\n",
      "< <pad> <pad> <pad>\n",
      "\n",
      "> ['để', 'tưởng_nhớ', 'tới', 'vụ', 'tấn_công', 'ngày', '26/11', 'hơn', 'một', 'năm', 'trước', ',', 'chúng_tôi', 'đã', 'giúp_đỡ', 'một', 'tổ_chức', 'phi', 'chính_phủ', 'ở', 'pakistan', ',', 'quỹ', 'aman', ',', 'xây_dựng', 'nên', 'một', 'đội', 'xe', 'cứu_thương', 'có', 'thể', 'tự', 'hoạt_động', 'ở', 'karachi', ',', 'được', 'tạo', 'điều_kiện', 'bởi', 'acumen', 'fund', '.']\n",
      "= ['in', 'tribute', 'and', 'remembrance', 'of', '26', '/', '11', 'attacks', 'over', 'the', 'last', 'one', 'year', ',', 'we', 'have', 'actually', 'helped', 'a', 'pakistani', 'ngo', ',', 'aman', 'foundation', ',', 'to', 'set', 'up', 'a', 'self-sustainable', 'life', 'support', 'ambulance', 'service', 'in', 'karachi', ',', 'facilitated', 'by', 'acumen', 'fund', '.']\n",
      "< <pad> , <pad>\n",
      "\n",
      "> ['dĩ_nhiên', ',', 'bạn', 'có_thể', '\"', 'green', '\"', 'nhiều', 'thứ', '.']\n",
      "= ['and', 'of', 'course', ',', 'you', 'can', '&quot;', 'green', '&quot;', 'things', '.']\n",
      "< <pad> <pad>\n",
      "\n",
      "> ['và', 'chúng_tôi', 'làm', 'một', 'cái', 'tương_tự', 'nữa', '.']\n",
      "= ['and', 'then', 'we', 'did', 'one', 'more', 'of', 'these', '.']\n",
      "< <pad> , <pad>\n",
      "\n",
      "> ['và', 'bạn', 'sẽ', 'suy_đoán', 'rằng', 'những', 'người_yêu', 'legos', 'sẽ', 'lắp_ráp', 'nhiều', 'mô_hình', 'legos', 'hơn', ',', 'thậm_chí', 'với', 'ít', 'tiền', 'hơn', ',', 'vì', 'trên_hết', ',', 'họ', 'có', 'thêm', 'nhiều', 'niềm_vui', 'cho', 'chính', 'bản', 'thân', 'bạn', '.']\n",
      "= ['and', 'you', 'would', 'speculate', 'that', 'the', 'people', 'who', 'love', 'legos', 'will', 'build', 'more', 'legos', ',', 'even', 'for', 'less', 'money', ',', 'because', 'after', 'all', ',', 'they', 'get', 'more', 'internal', 'joy', 'from', 'it', '.']\n",
      "< <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['nàng', 'tiến', 'gần', 'lại', 'chừng', 'đó', ',', 'và', 'rồi', 'bứt', 'ra', 'xa', '.']\n",
      "= ['she', 'gets', 'about', 'that', 'close', 'and', 'then', 'she', 'pulls', 'away', '.']\n",
      "< <pad> <pad> <pad>\n",
      "\n",
      "> ['có', 'ai', 'có', 'vấn_đề', 'nếu', 'phải', 'hôn', 'người_mẫu', 'để', 'thử', 'son', 'không', '?']\n",
      "= ['would', 'you', 'have', 'any', 'problem', 'kissing', 'our', 'models', 'to', 'test', 'it', '?']\n",
      "< <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = VI_EN_MAX_LENGTH\n",
    "evaluateRandomly(encoder1, decoder1, \"Vietnamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = randint(0, len(train_vi_vi_orig))\n",
    "sentence = train_vi_vi_orig[index] \n",
    "evaluateAndShowAttention(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate BLEU score on corpus level\n",
    "def BLEU(encoder, decoder, language):\n",
    "    hypotheses = \"\"\n",
    "    targets = \"\"\n",
    "    if language == 'Vietnamese':\n",
    "        inputs = val_vi_vi_orig\n",
    "        labels = val_vi_en_orig\n",
    "    else:\n",
    "        inputs = val_zh_zh_orig\n",
    "        labels = val_zh_zh_orig\n",
    "        \n",
    "    for sentence in inputs:\n",
    "        output = evaluate(encoder, decoder, sentence, language)\n",
    "        for word in output:\n",
    "            hypotheses = hypotheses + \" \" + word\n",
    "\n",
    "    #targets\n",
    "    for sentence in labels:\n",
    "        for word in sentence:\n",
    "            #replace infrequent words with <unk>\n",
    "            if word in en_id2token:\n",
    "                targets = targets + \" \" + word\n",
    "            else:\n",
    "                targets = targets + \" \" + '<unk>'\n",
    "\n",
    "    # hypotheses = hypotheses + (' '.join(train_vi_vi_orig[i])) + \" \"\n",
    "    # targets = targets + (' '.join(train_vi_en_orig[i])) + \" \"\n",
    "    score = sacrebleu.corpus_bleu(hypotheses, targets)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU(encoder1, decoder1, 'Vietnamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####OLD\n",
    " def forwardo(self, input, hidden, encoder_outputs):\n",
    "       # input.unsqueeze(0)\n",
    "        #embedded = self.dropout(self.embedding(input)).unsqueeze(0)\n",
    "        print(input)\n",
    "        print(hidden[0].size())\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)#.unsqueeze(0)\n",
    "        print(embedded[0].size())\n",
    "        print(torch.cat((embedded[0], hidden[0]), 1).size())\n",
    "        print(self.max_length)\n",
    "        print(encoder_outputs.size())\n",
    "        \n",
    "       # attn_weights = F.softmax(\n",
    "       #     self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        caten = torch.cat((embedded[0], hidden[0]), 1)\n",
    "#        line = self.attn(caten)\n",
    "#        attn_weights = F.softmax(\n",
    "#            line, self.max_length)\n",
    "#        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "       # Calculate attention weights and apply to encoder outputs\n",
    "       # attn_weights = self.attn(hidden.squeeze(0), encoder_outputs)\n",
    "       # context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "#        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "#        output = F.relu(output)\n",
    "#        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#        return output, hidden, attn_weights\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = F.softmax(\n",
    "            caten, dim=1)\n",
    "        print(attn_weights.size())\n",
    "        attn_applied = attn_weights.view(1, -1).mm(encoder_outputs.view(1, -1))\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(attn_weights.size())\n",
    "        print(embedded.squeeze(0).size())\n",
    "        print(hidden.squeeze(0).size())\n",
    "        print(embedded.view(1, 1, -1).size())\n",
    "        print(torch.cat((embedded, hidden), dim=1).size())\n",
    "        print(F.softmax(\n",
    "            torch.cat((embedded[0], hidden[0]), dim=1), dim=1).size())\n",
    "        print(F.softmax(self.attn(torch.cat((embedded.squeeze(0), hidden.squeeze(0)), 1))).size())\n",
    "        attn_weights =  F.softmax(self.attn(torch.cat((embedded.squeeze(0), hidden.squeeze(0)), 1)))\n",
    "        print(attn_weights.view(1, 1, -1).size())\n",
    "        print(encoder_outputs.view(1, 1, -1).size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         #attn_weights = self.attn(hidden[-1], encoder_outputs)\n",
    "        attn_weights = F.tanh(hidden, encoder_outputs)\n",
    "        print(attn_weights.size())\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        #attn_weights = F.softmax(\n",
    "        #    torch.cat((embedded[0], hidden[0]), dim=1), dim=1)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        #attn_weights = self.attn(torch.cat((embedded.squeeze(0), hidden.squeeze(0)), 1))\n",
    "        #attn_weights = self.attn(hidden.squeeze(0), encoder_outputs)\n",
    "        score = hidden.bmm(encoder_outputs)\n",
    "        print(score.size())\n",
    "        caten = torch.cat((embedded[0], hidden[0]), 1)\n",
    "        print(caten[-1].unsqueeze(0).size())\n",
    "        print(caten[-1].unsqueeze(1).size())\n",
    "        #attn = self.attn(caten)\n",
    "        attn_weights = F.softmax(caten, dim=1)\n",
    "        attn_weights = self.attn(attn_weights)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #attn_weights = self.attn(hidden[-1], encoder_outputs[-1])\n",
    "        #attn_weights = torch.cat((encoder_outputs.view(1, -1), attn_weights), dim=1)\n",
    "        attn_weights = F.softmax(attn_weights)\n",
    "        print(attn_weights.view(1, 1, -1).size())\n",
    "        print(encoder_outputs.view(1, 1,-1).size())\n",
    "        \n",
    "        context = torch.bmm(attn_weights.view(1, 1, -1),\n",
    "                                 attn_weights.view(1, 1, -1))\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        \n",
    "        # Get context vector\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                # Get weights from dot product\n",
    "        attn_weights = torch.bmm(hidden, encoder_outputs.view(1, -1, self.hidden_size).transpose(1,2))\n",
    "        print(hidden.size())\n",
    "        print(attn_weights.size())\n",
    "        # Softmax to normalize\n",
    "        context = F.softmax(attn_weights)\n",
    "        print(context.size())\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "\n",
    "        #context = self.attn(attn_weights, encoder_outputs)\n",
    "        \n",
    "        output = torch.cat((embedded, context), 2)\n",
    "        #output = self.attn_combine(output).unsqueeze(0)\n",
    "        print(output.size())\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(VI_EN_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
